"""
å¤„ç†ç”¨æˆ·æŸ¥è¯¢ï¼Œé€‰æ‹©åˆé€‚å·¥å…·ï¼Œæ‰§è¡Œå¹¶ç”Ÿæˆç­”æ¡ˆ
"""

import os
import json
from pathlib import Path
from datetime import datetime
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from tool_manager import UrbanToolManager


def load_env():
    """åŠ è½½ç¯å¢ƒå˜é‡"""
    env_file = Path(__file__).parent / '.env'
    if env_file.exists():
        try:
            from dotenv import load_dotenv
            load_dotenv(env_file)
            print("âœ… Environment variables loaded from .env")
        except ImportError:
            print("âš ï¸  python-dotenv not installed, using system environment variables")
    else:
        print("â„¹ï¸  No .env file found, using system environment variables")


def simple_tool_selection(query: str, tool_manager: UrbanToolManager) -> dict:
    """
    ç®€å•çš„å·¥å…·é€‰æ‹©ï¼ˆå•æ­¥ä»»åŠ¡ï¼‰
    ä½¿ç”¨ LLM ä»å·¥å…·æ± ä¸­é€‰æ‹©æœ€åˆé€‚çš„å·¥å…·

    Args:
        query: ç”¨æˆ·æŸ¥è¯¢
        tool_manager: å·¥å…·ç®¡ç†å™¨

    Returns:
        é€‰æ‹©ç»“æœ {"tool_name": "...", "reasoning": "...", "parameters": {...}}
    """
    llm = ChatOpenAI(
        model=os.getenv("OPENAI_MODEL", "gpt-4o"),
        temperature=0,
        api_key=os.getenv("OPENAI_API_KEY"),
        base_url=os.getenv("OPENAI_BASE_URL")
    )

    # è·å–å·¥å…·æè¿°
    tools_desc = tool_manager.get_tools_description()

    prompt = ChatPromptTemplate.from_messages([
        ("system", """You are an urban computing expert.
Given a user query, select the most suitable tool from the available tools.

Available Tools:
{tools}

Analyze the query and select the best tool to answer it.

IMPORTANT PARAMETER EXTRACTION RULES:
- For GitHub usernames: convert to lowercase, remove spaces (e.g., "Linus Torvalds" -> "torvalds")
- For weather_forecast (RapidAPI): Use "place" parameter with format "City,CountryCode" (e.g., "London,GB", "Beijing,CN", "Tokyo,JP")
- For weather_forecast_free (Open-Meteo): Use "latitude" and "longitude" parameters with decimal coordinates
- For coordinates queries: use the provided latitude/longitude values if available, otherwise use weather_forecast_free with location name
- Extract parameter values in the exact format expected by the tool

Respond ONLY with valid JSON in this exact format:
{{
  "tool_name": "exact_tool_name_from_list",
  "reasoning": "brief explanation of why this tool is suitable",
  "parameters": {{"param1": "value1", "param2": "value2"}}
}}

Do not include any markdown formatting or code blocks, just the raw JSON."""),
        ("user", "{query}")
    ])

    chain = prompt | llm
    response = chain.invoke({"query": query, "tools": tools_desc})

    # è§£æ JSON
    try:
        # æ¸…ç†å¯èƒ½çš„ markdown ä»£ç å—æ ‡è®°
        content = response.content.strip()
        if content.startswith("```"):
            # ç§»é™¤ ```json å’Œ ```
            content = content.split("```")[1]
            if content.startswith("json"):
                content = content[4:].strip()

        result = json.loads(content)
        return result
    except json.JSONDecodeError as e:
        print(f"âŒ Failed to parse LLM response as JSON: {e}")
        print(f"Raw response: {response.content}")
        raise


def generate_final_answer(query: str, tool_name: str, tool_result: dict) -> str:
    """
    ç”¨ LLM ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ

    Args:
        query: ç”¨æˆ·æŸ¥è¯¢
        tool_name: ä½¿ç”¨çš„å·¥å…·åç§°
        tool_result: å·¥å…·æ‰§è¡Œç»“æœ

    Returns:
        æœ€ç»ˆç­”æ¡ˆæ–‡æœ¬
    """
    llm = ChatOpenAI(
        model=os.getenv("OPENAI_MODEL", "gpt-4o"),
        temperature=0.3,
        api_key=os.getenv("OPENAI_API_KEY"),
        base_url=os.getenv("OPENAI_BASE_URL")
    )

    answer_prompt = ChatPromptTemplate.from_messages([
        ("system", """You are an urban computing expert.
Generate a clear, comprehensive answer based on the tool execution result.

If the tool execution failed (success=False), explain the error to the user in a friendly way.
If the tool succeeded, interpret and present the results in a user-friendly format."""),
        ("user", """User Query: {query}

Tool Used: {tool_name}
Tool Result: {result}

Please provide a comprehensive answer to the user's query based on this result.""")
    ])

    chain = answer_prompt | llm
    final_answer = chain.invoke({
        "query": query,
        "tool_name": tool_name,
        "result": json.dumps(tool_result, indent=2)
    })

    return final_answer.content


def process_query(query: str, tool_manager: UrbanToolManager) -> dict:
    """
    å¤„ç†å•æ­¥æŸ¥è¯¢çš„ä¸»æµç¨‹

    Args:
        query: ç”¨æˆ·æŸ¥è¯¢
        tool_manager: å·¥å…·ç®¡ç†å™¨

    Returns:
        å¤„ç†ç»“æœå­—å…¸
    """
    print(f"\n{'='*60}")
    print(f"ğŸ” Query: {query}")
    print(f"{'='*60}\n")

    # 1. é€‰æ‹©å·¥å…·
    print("ğŸ¤” Selecting appropriate tool...")
    selection = simple_tool_selection(query, tool_manager)

    print(f"âœ… Selected: {selection['tool_name']}")
    print(f"ğŸ’¡ Reasoning: {selection['reasoning']}")
    print(f"ğŸ“‹ Parameters: {json.dumps(selection['parameters'], indent=2)}")

    # 2. æ‰§è¡Œå·¥å…·
    tool = tool_manager.get_tool_by_name(selection['tool_name'])
    if not tool:
        error_msg = f"Tool '{selection['tool_name']}' not found in tool pool"
        print(f"âŒ Error: {error_msg}")
        return {
            "query": query,
            "tool_used": selection['tool_name'],
            "tool_result": {"success": False, "error": error_msg},
            "final_answer": f"Error: {error_msg}"
        }

    print(f"\nâš™ï¸  Executing tool '{selection['tool_name']}'...")
    tool_result = tool.invoke(selection['parameters'])

    if tool_result.get("success"):
        print("âœ… Tool execution succeeded")
    else:
        print(f"âŒ Tool execution failed: {tool_result.get('error')}")

    # 3. ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
    print("\nğŸ¤– Generating final answer...")
    final_answer = generate_final_answer(query, selection['tool_name'], tool_result)

    return {
        "query": query,
        "tool_used": selection['tool_name'],
        "tool_result": tool_result,
        "final_answer": final_answer
    }


def main():
    """ä¸»å‡½æ•°"""
    # åŠ è½½ç¯å¢ƒå˜é‡
    load_env()

    # éªŒè¯å¿…è¦çš„ç¯å¢ƒå˜é‡
    if not os.getenv("OPENAI_API_KEY"):
        print("âŒ Error: OPENAI_API_KEY not set")
        print("Please set it in .env file or environment variables")
        return

    # åˆå§‹åŒ–å·¥å…·ç®¡ç†å™¨
    try:
        tool_manager = UrbanToolManager("./urban_tools.json")
        print(f"\nğŸ“¦ Loaded {len(tool_manager.get_tools())} tools from pool\n")
    except FileNotFoundError as e:
        print(f"âŒ Error: {e}")
        return

    # ç¤ºä¾‹æŸ¥è¯¢ï¼ˆä½¿ç”¨å®é™…å¯ç”¨çš„ API å·¥å…·ï¼‰
    queries = [
        "Get GitHub user information for Linus Torvalds",
        "What is the weather forecast for Beijing? (coordinates: 39.9042Â°N, 116.4074Â°E)",
    ]

    # å¤„ç†æŸ¥è¯¢
    results = []
    for query in queries:
        try:
            result = process_query(query, tool_manager)
            results.append(result)

            # æ‰“å°æœ€ç»ˆç­”æ¡ˆ
            print(f"\nğŸ“ Final Answer:")
            print(f"{result['final_answer']}")
            print(f"\n{'='*60}\n")

        except Exception as e:
            print(f"âŒ Error processing query: {str(e)}")
            import traceback
            traceback.print_exc()

    # ä¿å­˜ç»“æœåˆ° Test æ–‡ä»¶å¤¹ï¼Œå¸¦æ—¶é—´æˆ³
    test_dir = Path(__file__).parent / "Test"
    test_dir.mkdir(exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = test_dir / f"results_{timestamp}.json"

    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    print(f"ğŸ’¾ Results saved to {output_file}")


if __name__ == "__main__":
    main()
